{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement ipynb_import_lib (from versions: none)\n",
      "ERROR: No matching distribution found for ipynb_import_lib\n"
     ]
    }
   ],
   "source": [
    "pip install ipynb_import_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipynb_import_lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-417c517c26fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mipynb_import_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mch07\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mipynb_import_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_ipynb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./ch07.ipynb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ipynb_import_lib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ipynb_import_lib\n",
    "ch07 = ipynb_import_lib.import_ipynb(\"./ch07.ipynb\")\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self,W,b,stride=1,pad=0):\n",
    "        \"\"\"Convolutionレイヤー\n",
    "\n",
    "        Args:\n",
    "            W (numpy.ndarray): フィルター（重み）、形状は(FN, C, FH, FW)。\n",
    "            b (numpy.ndarray): バイアス、形状は(FN)。\n",
    "            stride (int, optional): ストライド、デフォルトは1。\n",
    "            pad (int, optional): パディング、デフォルトは0。\n",
    "        \"\"\"\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.dW = None      # 重みの微分値\n",
    "        self.db = None      # バイアスの微分値\n",
    "\n",
    "        self.x = None       # 逆伝播で必要になる、順伝播時の入力\n",
    "        self.col_x = None   # 逆伝播で必要になる、順伝播時の入力のcol展開結果\n",
    "        self.col_W = None   # 逆伝播で必要になる、順伝播時のフィルターのcol展開結果\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"順伝播\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray): 入力。形状は(N, C, H, W)。\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: 出力。形状は(N, FN, OH, OW)。\n",
    "        \"\"\"\n",
    "        FN,C,FH,FW = self.W.shape  # FN:フィルター数、C:チャンネル数、FH:フィルターの高さ、FW:幅\n",
    "        N,x_C,H,W = x.shape        # N:バッチサイズ、x_C:チャンネル数、H：入力データの高さ、W:幅\n",
    "        # 自分で設定したエラーをはいて停止できる\n",
    "        assert C == x_C, f'チャンネル数の不一致！[C]{C}, [x_C]{x_C}'\n",
    "\n",
    "        # 出力のサイズ算出\n",
    "        assert (H + 2 * self.pad - FH) % self.stride == 0, 'OHが割り切れない！'\n",
    "        assert (W + 2 * self.pad - FW) % self.stride == 0, 'OWが割り切れない！'\n",
    "        OH = int((H + 2 * self.pad - FH) / self.stride + 1)\n",
    "        OW = int((W + 2 * self.pad - FW) / self.stride + 1)\n",
    "\n",
    "        # 入力データを展開\n",
    "        # (N, C, H, W) → (N * OH * OW, C * FH * FW)\n",
    "        col_x = im2col(x, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        # フィルターを展開(.Tで転置)\n",
    "        # (FN, C, FH, FW) → (FN, C * FH * FW)→(C * FH * FW, FN)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        # 出力を算出（col_x, col_W, bに対する計算は、Affineレイヤーと全く同じ）\n",
    "        # (N * OH * OW, C * FH * FW)・(C * FH * FW, FN) → (N * OH * OW, FN)\n",
    "        out = np.dot(col_x, col_W) + self.b\n",
    "\n",
    "        # 結果の整形\n",
    "        # (N * OH * OW, FN) → (N, OH, OW, FN) → (N, FN, OH, OW)\n",
    "        out = out.reshape(N, OH, OW, FN).transpose(0, 3, 1, 2)\n",
    "\n",
    "        # 逆伝播のために保存\n",
    "        self.x = x\n",
    "        self.col_x = col_x\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"逆伝播\n",
    "\n",
    "        Args:\n",
    "            dout (numpy.ndarray): 右の層から伝わってくる微分値、形状は(N, FN, OH, OW)。\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: 微分値（勾配）、形状は(N, C, H, W)。\n",
    "        \"\"\"\n",
    "        FN,C,FH,FW = self.W.shape  # 微分値の形状はWと同じ(FN, C, FH, FW)\n",
    "\n",
    "        # 右の層からの微分値を展開\n",
    "        # (N, FN, OH, OW) → (N, OH, OW, FN) → (N * OH * OW, FN)\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
    "\n",
    "        # 微分値算出（col_x, col_W, bに対する計算は、Affineレイヤーと全く同じ）\n",
    "        dcol_x = np.dot(dout, self.col_W.T)     # → (N * OH * OW, C * FH * FW)\n",
    "        self.dW = np.dot(self.col_x.T, dout)    # → (C * FH * FW, FN)\n",
    "        self.db = np.sum(dout, axis=0)          # → (FN)\n",
    "\n",
    "        # フィルター（重み）の微分値の整形\n",
    "        # (C * FH * FW, FN) → (FN, C * FH * FW) → (FN, C, FH, FW)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        # 結果（勾配）の整形\n",
    "        # (N * OH * OW, C * FH * FW) → (N, C, H, W)\n",
    "        dx = col2im(dcol_x, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        \"\"\"Poolingレイヤー\n",
    "\n",
    "        Args:\n",
    "            pool_h (int): プーリング領域の高さ\n",
    "            pool_w (int): プーリング領域の幅\n",
    "            stride (int, optional): ストライド、デフォルトは1。\n",
    "            pad (int, optional): パディング、デフォルトは0。\n",
    "        \"\"\"\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.x = None           # 逆伝播で必要になる、順伝播時の入力\n",
    "        self.arg_max = None     # 逆伝播で必要になる、順伝播時に採用したcol_x各行の位置\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"順伝播\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray): 入力、形状は(N, C, H, W)。\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: 出力、形状は(N, C, OH, OW)。\n",
    "        \"\"\"\n",
    "        N, C, H, W = x.shape  # N:データ数、C:チャンネル数、H:高さ、W:幅\n",
    "\n",
    "        # 出力のサイズ算出\n",
    "        assert (H - self.pool_h) % self.stride == 0, 'OHが割り切れない！'\n",
    "        assert (W - self.pool_w) % self.stride == 0, 'OWが割り切れない！'\n",
    "        OH = int((H - self.pool_h) / self.stride + 1)\n",
    "        OW = int((W - self.pool_w) / self.stride + 1)\n",
    "\n",
    "        # 入力データを展開、整形\n",
    "        # (N, C, H, W) → (N * OH * OW, C * PH * PW)\n",
    "        col_x = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        # (N * OH * OW, C * PH * PW) → (N * OH * OW * C, PH * PW)\n",
    "        col_x = col_x.reshape(-1, self.pool_h * self.pool_w)\n",
    "\n",
    "        # 出力(最大値)を算出\n",
    "        # (N * OH * OW * C, PH * PW) → (N * OH * OW * C)\n",
    "        out = np.max(col_x, axis=1)\n",
    "\n",
    "        # 出力の整形\n",
    "        # (N * OH * OW * C) → (N, OH, OW, C) → (N, C, OH, OW)\n",
    "        out = out.reshape(N, OH, OW, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        # 逆伝播のために保存\n",
    "        self.x = x\n",
    "        self.arg_max = np.argmax(col_x, axis=1)  # col_x各行の最大値の位置（インデックス）\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"逆伝播\n",
    "\n",
    "        Args:\n",
    "            dout (numpy.ndarray): 右の層から伝わってくる微分値、形状は(N, C, OH, OW)。\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: 微分値（勾配）、形状は(N, C, H, W)。\n",
    "        \"\"\"\n",
    "        # 右の層からの微分値を整形\n",
    "        # (N, C, OH, OW) → (N, OH, OW, C)\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "\n",
    "        # 結果の微分値用のcolを0で初期化\n",
    "        # (N * OH * OW * C, PH * PW)\n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dcol_x = np.zeros((dout.size, pool_size))\n",
    "\n",
    "        # 順伝播時に最大値として採用された位置にだけ、doutの微分値（＝doutまんま）をセット\n",
    "        # 順伝播時に採用されなかった値の位置は初期化時の0のまま\n",
    "        # （ReLUでxが0より大きい場合およびxが0以下の場合の処理と同じ）\n",
    "        assert dout.size == self.arg_max.size, '順伝搬時のcol_xの行数と合わない'\n",
    "        dcol_x[np.arange(self.arg_max.size), self.arg_max.flatten()] = \\\n",
    "            dout.flatten()\n",
    "\n",
    "        # 結果の微分値の整形1\n",
    "        # (N * OH * OW * C, PH * PW) → (N, OH, OW, C, PH * PW)\n",
    "        dcol_x = dcol_x.reshape(dout.shape + (pool_size,))  # 最後の','は1要素のタプルを示す\n",
    "\n",
    "        # 結果の微分値の整形2\n",
    "        # (N, OH, OW, C, PH * PW) → (N * OH * OW, C * PH * PW)\n",
    "        dcol_x = dcol_x.reshape(\n",
    "            dcol_x.shape[0] * dcol_x.shape[1] * dcol_x.shape[2], -1\n",
    "        )\n",
    "\n",
    "        # 結果の微分値の整形3\n",
    "        # (N * OH * OW, C * PH * PW) → (N, C, H, W)\n",
    "        dx = col2im(\n",
    "            dcol_x, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad\n",
    "        )\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLUレイヤー\n",
    "        \"\"\"\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"順伝播\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray): 入力\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: 出力\n",
    "        \"\"\"\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"逆伝播\n",
    "\n",
    "        Args:\n",
    "            dout (numpy.ndarray): 右の層から伝わってくる微分値\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: 微分値\n",
    "        \"\"\"\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "    \n",
    "# Affine\n",
    "class Affine:\n",
    "\n",
    "    def __init__(self, W, b):\n",
    "        \"\"\"Affineレイヤー\n",
    "\n",
    "        Args:\n",
    "            W (numpy.ndarray): 重み\n",
    "            b (numpy.ndarray): バイアス\n",
    "        \"\"\"\n",
    "        self.W = W                      # 重み\n",
    "        self.b = b                      # バイアス\n",
    "        self.x = None                   # 入力（2次元化後）\n",
    "        self.dW = None                  # 重みの微分値\n",
    "        self.db = None                  # バイアスの微分値\n",
    "        self.original_x_shape = None    # 元の入力の形状（3次元以上の入力時用）\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"順伝播\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray): 入力\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: 出力\n",
    "        \"\"\"\n",
    "        # 3次元以上（テンソル）の入力を2次元化\n",
    "        self.original_x_shape = x.shape  # 形状を保存、逆伝播で戻す必要があるので\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        # 出力を算出\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"逆伝播\n",
    "\n",
    "        Args:\n",
    "            dout (numpy.ndarray): 右の層から伝わってくる微分値\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: 微分値\n",
    "        \"\"\"\n",
    "        # 微分値算出\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        # 元の形状に戻す\n",
    "        dx = dx.reshape(*self.original_x_shape)\n",
    "        return dx\n",
    "\n",
    "# Softmax + 損失関数\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        \"\"\"Softmax-with-Lossレイヤー\n",
    "        \"\"\"\n",
    "        self.loss = None    # 損失\n",
    "        self.y = None       # softmaxの出力\n",
    "        self.t = None       # 教師データ（one-hot vector）\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"順伝播\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray): 入力\n",
    "            t (numpy.ndarray): 教師データ\n",
    "\n",
    "        Returns:\n",
    "            float: 交差エントロピー誤差\n",
    "        \"\"\"\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        \"\"\"逆伝播\n",
    "\n",
    "        Args:\n",
    "            dout (float, optional): 右の層から伝わってくる微分値。デフォルトは1。\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: 微分値\n",
    "        \"\"\"\n",
    "        batch_size = self.t.shape[0]    # バッチの個数\n",
    "        dx = (self.y - self.t) * (dout / batch_size)\n",
    "\n",
    "        return dx\n",
    "\n",
    "# ハイパーパラメータの更新\n",
    "class AdaGrad:\n",
    "\n",
    "    \"\"\"AdaGrad\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "#           1e-7で0除算を避けている\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "class RMSprop:\n",
    "\n",
    "    \"\"\"RMSprop\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "# Softmaxと交差エントロピー誤差メソッド\n",
    "def softmax(x):\n",
    "    \"\"\"ソフトマックス関数\n",
    "\n",
    "    Args:\n",
    "        x (numpy.ndarray): 入力\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: 出力\n",
    "    \"\"\"\n",
    "    # バッチ処理の場合xは(バッチの数, 10)の2次元配列になる。\n",
    "    # この場合、ブロードキャストを使ってうまく画像ごとに計算する必要がある。\n",
    "    # ここでは1次元でも2次元でも共通化できるようnp.max()やnp.sum()はaxis=-1で算出し、\n",
    "    # そのままブロードキャストできるようkeepdims=Trueで次元を維持する。\n",
    "    c = np.max(x, axis=-1, keepdims=True)\n",
    "    exp_a = np.exp(x - c)  # オーバーフロー対策\n",
    "    sum_exp_a = np.sum(exp_a, axis=-1, keepdims=True)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    \"\"\"交差エントロピー誤差の算出\n",
    "\n",
    "    Args:\n",
    "        y (numpy.ndarray): ニューラルネットワークの出力\n",
    "        t (numpy.ndarray): 正解のラベル\n",
    "\n",
    "    Returns:\n",
    "        float: 交差エントロピー誤差\n",
    "    \"\"\"\n",
    "\n",
    "    # データ1つ場合は形状を整形（1データ1行にする）\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 誤差を算出してバッチ数で正規化\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learningに必要なメソッドの準備\n",
    "# coding: utf-8\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        \"\"\"Dropoutレイヤー\n",
    "\n",
    "        Args:\n",
    "            dropout_ratio (float): 学習時のニューロンの消去割合、デフォルトは0.5。\n",
    "        \"\"\"\n",
    "        self.dropout_ratio = dropout_ratio              # 学習時のニューロンの消去割合\n",
    "        self.valid_ratio = 1.0 - self.dropout_ratio     # 学習時に生かしていた割合\n",
    "        self.mask = None                                # 各ニューロンの消去有無を示すフラグの配列\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        \"\"\"順伝播\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray): 入力\n",
    "            train_flg (bool, optional): 学習中ならTrue、デフォルトはTrue。\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: 出力\n",
    "        \"\"\"\n",
    "        if train_flg:\n",
    "            # 学習時は消去するニューロンを決めるマスクを生成\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "\n",
    "            # 出力を算出\n",
    "            return x * self.mask\n",
    "\n",
    "        else:\n",
    "            # 認識時はニューロンは消去しないが、学習時の消去割合を加味した出力に調整する\n",
    "            return x * self.valid_ratio\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"逆伝播\n",
    "\n",
    "        Args:\n",
    "            dout (numpy.ndarray): 右の層から伝わってくる微分値\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: 微分値（勾配）\n",
    "        \"\"\"\n",
    "        # 消去しなかったニューロンのみ右の層の微分値を逆伝播\n",
    "        assert self.mask is not None, '順伝播なしに逆伝播が呼ばれた'\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "\n",
    "    def __init__(self, alpha=0.001, beta1=0.9, beta2=0.999):\n",
    "        \"\"\"Adamによるパラメーターの最適化\n",
    "\n",
    "        Args:\n",
    "            alpha (float, optional): 学習係数、デフォルトは0.001。\n",
    "            beta1 (float, optional): Momentumにおける速度の過去と今の按分の係数、デフォルトは0.9。\n",
    "            beta2 (float, optional): AdaGradにおける学習係数の過去と今の按分の係数、デフォルトは0.999。\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "\n",
    "        self.m = None   # Momentumにおける速度\n",
    "        self.v = None   # AdaGradにおける学習係数\n",
    "        self.t = 0      # タイムステップ\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        \"\"\"パラメーター更新\n",
    "\n",
    "        Args:\n",
    "            params (dict): 更新対象のパラメーターの辞書、keyは'W1'、'b1'など。\n",
    "            grads (dict): paramsに対応する勾配の辞書\n",
    "        \"\"\"\n",
    "        # mとvの初期化\n",
    "        if self.m is None:\n",
    "            self.m = {}\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "\n",
    "        # 更新\n",
    "        self.t += 1     # タイムステップ加算\n",
    "        for key in params.keys():\n",
    "\n",
    "            # mの更新、Momentumにおける速度の更新に相当\n",
    "            # 過去と今の勾配を beta1 : 1 - beta1 で按分する\n",
    "            self.m[key] = \\\n",
    "                self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n",
    "\n",
    "            # vの更新、AdaGradにおける学習係数の更新に相当\n",
    "            # 過去と今の勾配を beta2 : 1 - beta2 で按分する\n",
    "            self.v[key] = \\\n",
    "                self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)\n",
    "\n",
    "            # パラメーター更新のためのmとvの補正値算出\n",
    "            hat_m = self.m[key] / (1.0 - self.beta1 ** self.t)\n",
    "            hat_v = self.v[key] / (1.0 - self.beta2 ** self.t)\n",
    "\n",
    "            # パラメーター更新、最後の1e-7は0除算回避\n",
    "            params[key] -= self.alpha * hat_m / (np.sqrt(hat_v) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"ソフトマックス関数\n",
    "\n",
    "    Args:\n",
    "        x (numpy.ndarray): 入力\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: 出力\n",
    "    \"\"\"\n",
    "    # バッチ処理の場合xは(バッチの数, 10)の2次元配列になる。\n",
    "    # この場合、ブロードキャストを使ってうまく画像ごとに計算する必要がある。\n",
    "    # ここでは1次元でも2次元でも共通化できるようnp.max()やnp.sum()はaxis=-1で算出し、\n",
    "    # そのままブロードキャストできるようkeepdims=Trueで次元を維持する。\n",
    "    c = np.max(x, axis=-1, keepdims=True)\n",
    "    exp_a = np.exp(x - c)  # オーバーフロー対策\n",
    "    sum_exp_a = np.sum(exp_a, axis=-1, keepdims=True)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    \"\"\"交差エントロピー誤差の算出\n",
    "\n",
    "    Args:\n",
    "        y (numpy.ndarray): ニューラルネットワークの出力\n",
    "        t (numpy.ndarray): 正解のラベル\n",
    "\n",
    "    Returns:\n",
    "        float: 交差エントロピー誤差\n",
    "    \"\"\"\n",
    "\n",
    "    # データ1つ場合は形状を整形（1データ1行にする）\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 誤差を算出してバッチ数で正規化\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "def conv_output_size(input_size, filter_size, pad, stride):\n",
    "    \"\"\"畳み込み層の出力サイズ算出\n",
    "\n",
    "    Args:\n",
    "        input_size (int): 入力の1辺のサイズ（縦横は同値の前提）\n",
    "        filter_size (int): フィルターの1辺のサイズ（縦横は同値の前提）\n",
    "        pad (int): パディングのサイズ（縦横は同値の前提）\n",
    "        stride (int): ストライド幅（縦横は同値の前提）\n",
    "\n",
    "    Returns:\n",
    "        int: 出力の1辺のサイズ\n",
    "    \"\"\"\n",
    "    assert (input_size + 2 * pad - filter_size) \\\n",
    "        % stride == 0, '畳み込み層の出力サイズが割り切れない！'\n",
    "    return int((input_size + 2 * pad - filter_size) / stride + 1)\n",
    "\n",
    "\n",
    "def pool_output_size(input_size, pool_size, stride):\n",
    "    \"\"\"プーリング層の出力サイズ算出\n",
    "\n",
    "    Args:\n",
    "        input_size (int): 入力の1辺のサイズ（縦横は同値の前提）\n",
    "        pool_size (int): プーリングのウインドウサイズ（縦横は同値の前提）\n",
    "        stride (int): ストライド幅（縦横は同値の前提）\n",
    "\n",
    "    Returns:\n",
    "        int: 出力の1辺のサイズ\n",
    "    \"\"\"\n",
    "    assert (input_size - pool_size) % stride == 0, 'プーリング層の出力サイズが割り切れない！'\n",
    "    return int((input_size - pool_size) / stride + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im2colメソッドの中身→そういうものとして使う：順伝播\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n",
    "    filter_h : フィルターの高さ\n",
    "    filter_w : フィルターの幅\n",
    "    stride : ストライド\n",
    "    pad : パディング\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2次元配列\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "# 逆伝播の時に使う\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : 入力データの形状（例：(10, 1, 28, 28)）\n",
    "    filter_h :\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディープなCNNの実装\n",
    "# coding: utf-8\n",
    "import numpy as np\n",
    "\n",
    "class DeepConvNet:\n",
    "\n",
    "    def __init__(\n",
    "        self, input_dim=(1, 28, 28),\n",
    "        conv_param_1={\n",
    "            'filter_num': 16, 'filter_size': 3, 'pad': 1, 'stride': 1\n",
    "        },\n",
    "        conv_param_2={\n",
    "            'filter_num': 16, 'filter_size': 3, 'pad': 1, 'stride': 1\n",
    "        },\n",
    "        conv_param_3={\n",
    "            'filter_num': 32, 'filter_size': 3, 'pad': 1, 'stride': 1\n",
    "        },\n",
    "        conv_param_4={\n",
    "            'filter_num': 32, 'filter_size': 3, 'pad': 2, 'stride': 1\n",
    "        },\n",
    "        conv_param_5={\n",
    "            'filter_num': 64, 'filter_size': 3, 'pad': 1, 'stride': 1\n",
    "        },\n",
    "        conv_param_6={\n",
    "            'filter_num': 64, 'filter_size': 3, 'pad': 1, 'stride': 1\n",
    "        },\n",
    "        hidden_size=50, output_size=10\n",
    "    ):\n",
    "        \"\"\"ディープな畳み込みニューラルネットワーク\n",
    "\n",
    "        Args:\n",
    "            input_dim (tuple, optional): 入力データの形状、デフォルトは(1, 28, 28)。\n",
    "            conv_param_1 (dict, optional): 畳み込み層1のハイパーパラメーター、\n",
    "                デフォルトは{'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1}。\n",
    "            conv_param_2 (dict, optional): 畳み込み層2のハイパーパラメーター、\n",
    "                デフォルトは{'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1}。\n",
    "            conv_param_3 (dict, optional): 畳み込み層3のハイパーパラメーター、\n",
    "                デフォルトは{'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1}。\n",
    "            conv_param_4 (dict, optional): 畳み込み層4のハイパーパラメーター、\n",
    "                デフォルトは{'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1}。\n",
    "            conv_param_5 (dict, optional): 畳み込み層5のハイパーパラメーター、\n",
    "                デフォルトは{'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1}。\n",
    "            conv_param_6 (dict, optional): 畳み込み層6のハイパーパラメーター、\n",
    "                デフォルトは{'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1}。\n",
    "            hidden_size (int, optional): 隠れ層のニューロンの数、デフォルトは50。\n",
    "            output_size (int, optional): 出力層のニューロンの数、デフォルトは10。\n",
    "        \"\"\"\n",
    "        assert input_dim[1] == input_dim[2], '入力データは高さと幅が同じ前提！'\n",
    "\n",
    "        # パラメーターの初期化とレイヤー生成\n",
    "        self.params = {}    # パラメーター\n",
    "        self.layers = {}    # レイヤー（Python 3.7からは辞書の格納順が保持されるので、OrderedDictは不要）\n",
    "\n",
    "        # 入力サイズ\n",
    "        channel_num = input_dim[0]                          # 入力のチャンネル数\n",
    "        input_size = input_dim[1]                           # 入力サイズ\n",
    "\n",
    "        # [1] 畳み込み層#1 : パラメーター初期化とレイヤー生成\n",
    "        filter_num, filter_size, pad, stride = list(conv_param_1.values())\n",
    "        pre_node_num = channel_num * (filter_size ** 2)     # 1ノードに対する前層の接続ノード数\n",
    "        key_w, key_b = 'W1', 'b1'                           # 辞書格納時のkey\n",
    "        self.params[key_w] = np.random.normal(\n",
    "            scale=np.sqrt(2.0 / pre_node_num),              # Heの初期値の標準偏差\n",
    "            size=(filter_num, channel_num, filter_size, filter_size)\n",
    "        )\n",
    "        self.params[key_b] = np.zeros(filter_num)\n",
    "\n",
    "        self.layers['Conv1'] = Convolution(\n",
    "            self.params[key_w], self.params[key_b], stride, pad\n",
    "        )\n",
    "\n",
    "        # 次の層の入力サイズ算出\n",
    "        channel_num = filter_num\n",
    "        input_size = conv_output_size(input_size, filter_size, pad, stride)\n",
    "\n",
    "        # [2] ReLU層#1 : レイヤー生成\n",
    "        self.layers['ReLU1'] = ReLU()\n",
    "\n",
    "        # [3] 畳み込み層#2 : パラメーター初期化とレイヤー生成\n",
    "        filter_num, filter_size, pad, stride = list(conv_param_2.values())\n",
    "        pre_node_num = channel_num * (filter_size ** 2)     # 1ノードに対する前層の接続ノード数\n",
    "        key_w, key_b = 'W2', 'b2'                           # 辞書格納時のkey\n",
    "        self.params[key_w] = np.random.normal(\n",
    "            scale=np.sqrt(2.0 / pre_node_num),              # Heの初期値の標準偏差\n",
    "            size=(filter_num, channel_num, filter_size, filter_size)\n",
    "        )\n",
    "        self.params[key_b] = np.zeros(filter_num)\n",
    "\n",
    "        self.layers['Conv2'] = Convolution(\n",
    "            self.params[key_w], self.params[key_b], stride, pad\n",
    "        )\n",
    "\n",
    "        # 次の層の入力サイズ算出\n",
    "        channel_num = filter_num\n",
    "        input_size = conv_output_size(input_size, filter_size, pad, stride)\n",
    "\n",
    "        # [4] ReLU層#2 : レイヤー生成\n",
    "        self.layers['ReLU2'] = ReLU()\n",
    "\n",
    "        # [5] プーリング層#1 : レイヤー生成\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "\n",
    "        # 次の層の入力サイズ算出\n",
    "        input_size = pool_output_size(input_size, pool_size=2, stride=2)\n",
    "\n",
    "        # [6] 畳み込み層#3 : パラメーター初期化とレイヤー生成\n",
    "        filter_num, filter_size, pad, stride = list(conv_param_3.values())\n",
    "        pre_node_num = channel_num * (filter_size ** 2)     # 1ノードに対する前層の接続ノード数\n",
    "        key_w, key_b = 'W3', 'b3'                           # 辞書格納時のkey\n",
    "        self.params[key_w] = np.random.normal(\n",
    "            scale=np.sqrt(2.0 / pre_node_num),              # Heの初期値の標準偏差\n",
    "            size=(filter_num, channel_num, filter_size, filter_size)\n",
    "        )\n",
    "        self.params[key_b] = np.zeros(filter_num)\n",
    "\n",
    "        self.layers['Conv3'] = Convolution(\n",
    "            self.params[key_w], self.params[key_b], stride, pad\n",
    "        )\n",
    "\n",
    "        # 次の層の入力サイズ算出\n",
    "        channel_num = filter_num\n",
    "        input_size = conv_output_size(input_size, filter_size, pad, stride)\n",
    "\n",
    "        # [7] ReLU層#3 : レイヤー生成\n",
    "        self.layers['ReLU3'] = ReLU()\n",
    "\n",
    "        # [8] 畳み込み層#4 : パラメーター初期化とレイヤー生成\n",
    "        filter_num, filter_size, pad, stride = list(conv_param_4.values())\n",
    "        pre_node_num = channel_num * (filter_size ** 2)     # 1ノードに対する前層の接続ノード数\n",
    "        key_w, key_b = 'W4', 'b4'                           # 辞書格納時のkey\n",
    "        self.params[key_w] = np.random.normal(\n",
    "            scale=np.sqrt(2.0 / pre_node_num),              # Heの初期値の標準偏差\n",
    "            size=(filter_num, channel_num, filter_size, filter_size)\n",
    "        )\n",
    "        self.params[key_b] = np.zeros(filter_num)\n",
    "\n",
    "        self.layers['Conv4'] = Convolution(\n",
    "            self.params[key_w], self.params[key_b], stride, pad\n",
    "        )\n",
    "\n",
    "        # 次の層の入力サイズ算出\n",
    "        channel_num = filter_num\n",
    "        input_size = conv_output_size(input_size, filter_size, pad, stride)\n",
    "\n",
    "        # [9] ReLU層#4 : レイヤー生成\n",
    "        self.layers['ReLU4'] = ReLU()\n",
    "\n",
    "        # [10] プーリング層#2 : レイヤー生成\n",
    "        self.layers['Pool2'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "\n",
    "        # 次の層の入力サイズ算出\n",
    "        input_size = pool_output_size(input_size, pool_size=2, stride=2)\n",
    "\n",
    "        # [11] 畳み込み層#5 : パラメーター初期化とレイヤー生成\n",
    "        filter_num, filter_size, pad, stride = list(conv_param_5.values())\n",
    "        pre_node_num = channel_num * (filter_size ** 2)     # 1ノードに対する前層の接続ノード数\n",
    "        key_w, key_b = 'W5', 'b5'                           # 辞書格納時のkey\n",
    "        self.params[key_w] = np.random.normal(\n",
    "            scale=np.sqrt(2.0 / pre_node_num),              # Heの初期値の標準偏差\n",
    "            size=(filter_num, channel_num, filter_size, filter_size)\n",
    "        )\n",
    "        self.params[key_b] = np.zeros(filter_num)\n",
    "\n",
    "        self.layers['Conv5'] = Convolution(\n",
    "            self.params[key_w], self.params[key_b], stride, pad\n",
    "        )\n",
    "\n",
    "        # 次の層の入力サイズ算出\n",
    "        channel_num = filter_num\n",
    "        input_size = conv_output_size(input_size, filter_size, pad, stride)\n",
    "\n",
    "        # [12] ReLU層#5 : レイヤー生成\n",
    "        self.layers['ReLU5'] = ReLU()\n",
    "\n",
    "        # [13] 畳み込み層#6 : パラメーター初期化とレイヤー生成\n",
    "        filter_num, filter_size, pad, stride = list(conv_param_6.values())\n",
    "        pre_node_num = channel_num * (filter_size ** 2)     # 1ノードに対する前層の接続ノード数\n",
    "        key_w, key_b = 'W6', 'b6'                           # 辞書格納時のkey\n",
    "        self.params[key_w] = np.random.normal(\n",
    "            scale=np.sqrt(2.0 / pre_node_num),              # Heの初期値の標準偏差\n",
    "            size=(filter_num, channel_num, filter_size, filter_size)\n",
    "        )\n",
    "        self.params[key_b] = np.zeros(filter_num)\n",
    "\n",
    "        self.layers['Conv6'] = Convolution(\n",
    "            self.params[key_w], self.params[key_b], stride, pad\n",
    "        )\n",
    "\n",
    "        # 次の層の入力サイズ算出\n",
    "        channel_num = filter_num\n",
    "        input_size = conv_output_size(input_size, filter_size, pad, stride)\n",
    "\n",
    "        # [14] ReLU層#6 : レイヤー生成\n",
    "        self.layers['ReLU6'] = ReLU()\n",
    "\n",
    "        # [15] プーリング層#3 : レイヤー生成\n",
    "        self.layers['Pool3'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "\n",
    "        # 次の層の入力サイズ算出\n",
    "        input_size = pool_output_size(input_size, pool_size=2, stride=2)\n",
    "\n",
    "        # [16] Affine層#1　: パラメーター初期化とレイヤー生成\n",
    "        pre_node_num = channel_num * (input_size ** 2)      # 1ノードに対する前層の接続ノード数\n",
    "        key_w, key_b = 'W7', 'b7'                           # 辞書格納時のkey\n",
    "        self.params[key_w] = np.random.normal(\n",
    "            scale=np.sqrt(2.0 / pre_node_num),              # Heの初期値の標準偏差\n",
    "            size=(channel_num * (input_size ** 2), hidden_size)\n",
    "        )\n",
    "        self.params[key_b] = np.zeros(hidden_size)\n",
    "\n",
    "        self.layers['Affine1'] = Affine(self.params[key_w], self.params[key_b])\n",
    "\n",
    "        # 次の層の入力サイズ算出\n",
    "        input_size = hidden_size\n",
    "\n",
    "        # [17] ReLU層#7 : レイヤー生成\n",
    "        self.layers['ReLU7'] = ReLU()\n",
    "\n",
    "        # [18] Dropout層#１ : レイヤー生成\n",
    "        self.layers['Drop1'] = Dropout(dropout_ratio=0.5)\n",
    "\n",
    "        # [19] Affine層#2　: パラメーター初期化とレイヤー生成\n",
    "        pre_node_num = input_size                           # 1ノードに対する前層の接続ノード数\n",
    "        key_w, key_b = 'W8', 'b8'                           # 辞書格納時のkey\n",
    "        self.params[key_w] = np.random.normal(\n",
    "            scale=np.sqrt(2.0 / pre_node_num),              # Heの初期値の標準偏差\n",
    "            size=(input_size, output_size)\n",
    "        )\n",
    "        self.params[key_b] = np.zeros(output_size)\n",
    "\n",
    "        self.layers['Affine2'] = Affine(self.params[key_w], self.params[key_b])\n",
    "\n",
    "        # [20] Dropout層#2 : レイヤー生成\n",
    "        self.layers['Drop2'] = Dropout(dropout_ratio=0.5)\n",
    "\n",
    "        # [21] Softmax層 : レイヤー生成\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        \"\"\"ニューラルネットワークによる推論\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray): ニューラルネットワークへの入力\n",
    "            train_flg (Boolean): 学習中ならTrue（Dropout層でニューロンの消去を実施）\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: ニューラルネットワークの出力\n",
    "        \"\"\"\n",
    "        # レイヤーを順伝播\n",
    "        for layer in self.layers.values():\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)  # Dropout層の場合は、学習中かどうかを伝える\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"損失関数の値算出\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray): ニューラルネットワークへの入力\n",
    "            t (numpy.ndarray): 正解のラベル\n",
    "\n",
    "        Returns:\n",
    "            float: 損失関数の値\n",
    "        \"\"\"\n",
    "        # 推論\n",
    "        y = self.predict(x, True)   # 損失は学習中しか算出しないので常にTrue\n",
    "\n",
    "        # Softmax-with-Lossレイヤーの順伝播で算出\n",
    "        loss = self.lastLayer.forward(y, t)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        \"\"\"認識精度算出\n",
    "        batch_sizeは算出時のバッチサイズ。一度に大量データを算出しようとすると\n",
    "        im2colでメモリを食い過ぎてスラッシングが起きてしまい動かなくなるため、\n",
    "        その回避のためのもの。\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray): ニューラルネットワークへの入力\n",
    "            t (numpy.ndarray): 正解のラベル（one-hot）\n",
    "            batch_size (int), optional): 算出時のバッチサイズ、デフォルトは100。\n",
    "\n",
    "        Returns:\n",
    "            float: 認識精度\n",
    "        \"\"\"\n",
    "        # 分割数算出\n",
    "        batch_num = max(int(x.shape[0] / batch_size), 1)\n",
    "\n",
    "        # 分割\n",
    "        x_list = np.array_split(x, batch_num, 0)\n",
    "        t_list = np.array_split(t, batch_num, 0)\n",
    "\n",
    "        # 分割した単位で処理\n",
    "        correct_num = 0  # 正答数の合計\n",
    "        for (sub_x, sub_t) in zip(x_list, t_list):\n",
    "            assert sub_x.shape[0] == sub_t.shape[0], '分割境界がずれた？'\n",
    "            y = self.predict(sub_x, False)  # 認識精度は学習中は算出しないので常にFalse\n",
    "            y = np.argmax(y, axis=1)\n",
    "            t = np.argmax(sub_t, axis=1)\n",
    "            correct_num += np.sum(y == t)\n",
    "\n",
    "        # 認識精度の算出\n",
    "        return correct_num / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"重みパラメーターに対する勾配を誤差逆伝播法で算出\n",
    "\n",
    "         Args:\n",
    "            x (numpy.ndarray): ニューラルネットワークへの入力\n",
    "            t (numpy.ndarray): 正解のラベル\n",
    "\n",
    "        Returns:\n",
    "            dictionary: 勾配を格納した辞書\n",
    "        \"\"\"\n",
    "        # 順伝播\n",
    "        self.loss(x, t)     # 損失値算出のために順伝播する\n",
    "\n",
    "        # 逆伝播\n",
    "        dout = self.lastLayer.backward()\n",
    "        for layer in reversed(list(self.layers.values())):\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 各レイヤーの微分値を取り出し\n",
    "        grads = {}\n",
    "        layer = self.layers['Conv1']\n",
    "        grads['W1'], grads['b1'] = layer.dW, layer.db\n",
    "        layer = self.layers['Conv2']\n",
    "        grads['W2'], grads['b2'] = layer.dW, layer.db\n",
    "        layer = self.layers['Conv3']\n",
    "        grads['W3'], grads['b3'] = layer.dW, layer.db\n",
    "        layer = self.layers['Conv4']\n",
    "        grads['W4'], grads['b4'] = layer.dW, layer.db\n",
    "        layer = self.layers['Conv5']\n",
    "        grads['W5'], grads['b5'] = layer.dW, layer.db\n",
    "        layer = self.layers['Conv6']\n",
    "        grads['W6'], grads['b6'] = layer.dW, layer.db\n",
    "        layer = self.layers['Affine1']\n",
    "        grads['W7'], grads['b7'] = layer.dW, layer.db\n",
    "        layer = self.layers['Affine2']\n",
    "        grads['W8'], grads['b8'] = layer.dW, layer.db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-a4174c7b3423>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# 学習前の認識精度の確認\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mtrain_loss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m            \u001b[1;31m# 損失関数の値の推移の格納先\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-7628f572726a>\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(self, x, t, batch_size)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msub_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_t\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0msub_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msub_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'分割境界がずれた？'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 認識精度は学習中は算出しないので常にFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-7628f572726a>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, train_flg)\u001b[0m\n\u001b[0;32m    252\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_flg\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Dropout層の場合は、学習中かどうかを伝える\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-3720ea2f1a79>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m出力\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \"\"\"\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MNISTの訓練データとテストデータ読み込み\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, flatten=False, one_hot_label=True)\n",
    "\n",
    "# ハイパーパラメーター設定\n",
    "iters_num = 12000           # 更新回数\n",
    "batch_size = 100            # バッチサイズ\n",
    "adam_param_alpha = 0.001    # Adamのパラメーター\n",
    "adam_param_beta1 = 0.9      # Adamのパラメーター\n",
    "adam_param_beta2 = 0.999    # Adamのパラメーター\n",
    "\n",
    "train_size = x_train.shape[0]  # 訓練データのサイズ\n",
    "iter_per_epoch = max(int(train_size / batch_size), 1)    # 1エポック当たりの繰り返し数\n",
    "\n",
    "# ディープな畳み込みニューラルネットワーク生成\n",
    "network = DeepConvNet()\n",
    "\n",
    "# オプティマイザー生成、Adamを使用\n",
    "optimizer = Adam(adam_param_alpha, adam_param_beta1, adam_param_beta2)\n",
    "\n",
    "# 学習前の認識精度の確認\n",
    "train_acc = network.accuracy(x_train, t_train)\n",
    "test_acc = network.accuracy(x_test, t_test)\n",
    "train_loss_list = []            # 損失関数の値の推移の格納先\n",
    "train_acc_list = [train_acc]    # 訓練データに対する認識精度の推移の格納先\n",
    "test_acc_list = [test_acc]      # テストデータに対する認識精度の推移の格納先\n",
    "print(f'学習前 [訓練データの認識精度]{train_acc:.4f} [テストデータの認識精度]{test_acc:.4f}')\n",
    "\n",
    "# 学習開始\n",
    "for i in range(iters_num):\n",
    "\n",
    "    # ミニバッチ生成\n",
    "    batch_mask = np.random.choice(train_size, batch_size, replace=False)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 勾配の計算\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # 重みパラメーター更新\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    # 損失関数の値算出\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 1エポックごとに認識精度算出\n",
    "    if (i + 1) % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        # 経過表示\n",
    "        print(\n",
    "            f'[エポック]{(i + 1) // iter_per_epoch:>2} '\n",
    "            f'[更新数]{i + 1:>5} [損失関数の値]{loss:.4f} '\n",
    "            f'[訓練データの認識精度]{train_acc:.4f} [テストデータの認識精度]{test_acc:.4f}'\n",
    "        )\n",
    "\n",
    "# 損失関数の値の推移を描画\n",
    "x = np.arange(len(train_loss_list))\n",
    "plt.plot(x, train_loss_list, label='loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.xlim(left=0)\n",
    "plt.ylim(0, 2.5)\n",
    "plt.show()\n",
    "\n",
    "# 訓練データとテストデータの認識精度の推移を描画\n",
    "x2 = np.arange(len(train_acc_list))\n",
    "plt.plot(x2, train_acc_list, label='train acc')\n",
    "plt.plot(x2, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlim(left=0)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
